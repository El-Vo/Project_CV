{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2525a174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMv2 loaded on CPU\n",
      "Initializing tracker and SAM segmentation...\n",
      "Saved multi-view capture 000\n",
      "Saved multi-view capture 001\n",
      "Saved multi-view capture 002\n"
     ]
    }
   ],
   "source": [
    "#PART 1: SCANNING AN PERSONAL OBJECT AND SAVING OBJECT IN MULTI_VIEW FOLDER\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sam2 import load_model\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "\n",
    "# Load SAMv2 model (tiny for CPU)\n",
    "model = load_model(\n",
    "    variant=\"tiny\",\n",
    "    ckpt_path=\"models/sam2/checkpoints/sam2_hiera_tiny.pt\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "predictor = SAM2ImagePredictor(model)\n",
    "print(\"SAMv2 loaded on CPU\")\n",
    "\n",
    "# Webcam setup\n",
    "cap = cv2.VideoCapture(0)\n",
    "window_name = \"SAMv2 + Tracker + Multi-View Capture\"\n",
    "cv2.namedWindow(window_name)\n",
    "\n",
    "\n",
    "# Multi-view output folder\n",
    "output_folder = \"multi_view\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "capture_count = 0\n",
    "\n",
    "# Tracker initialization\n",
    "tracker_initialized = False\n",
    "tracker = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # Draw red center point (user selects object here)\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "    cv2.putText(frame, \"Press SPACE to start/capture\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    # Wait until first SPACE press to initialize tracker\n",
    "\n",
    "    if not tracker_initialized:\n",
    "        cv2.imshow(window_name, frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == 32:  \n",
    "            print(\"Initializing tracker and SAM segmentation...\")\n",
    "\n",
    "            predictor.set_image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            pts = np.array([[center_x, center_y]])\n",
    "            labels = np.array([1])\n",
    "            masks, _, _ = predictor.predict(\n",
    "                point_coords=pts,\n",
    "                point_labels=labels,\n",
    "                multimask_output=True\n",
    "            )\n",
    "            mask_bool = masks[0].astype(bool)\n",
    "\n",
    "            # Bounding box from mask\n",
    "            y_idx, x_idx = np.where(mask_bool)\n",
    "            x_min, x_max = x_idx.min(), x_idx.max()\n",
    "            y_min, y_max = y_idx.min(), y_idx.max()\n",
    "            bbox = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "\n",
    "            # Initialize tracker\n",
    "            tracker = cv2.TrackerCSRT_create()\n",
    "            tracker.init(frame, bbox)\n",
    "            tracker_initialized = True\n",
    "        elif key == 27:  # ESC\n",
    "            break\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Update tracker\n",
    "\n",
    "    success, bbox = tracker.update(frame)\n",
    "    if success:\n",
    "        x, y, w_box, h_box = [int(v) for v in bbox]\n",
    "        cv2.rectangle(frame, (x, y), (x + w_box, y + h_box), (255, 0, 0), 2)\n",
    "    else:\n",
    "        tracker_initialized = False\n",
    "        print(\"Tracker lost object. Press SPACE to reinitialize.\")\n",
    "        continue\n",
    "\n",
    "    cv2.imshow(window_name, frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\n",
    "    # Capture view on SPACE\n",
    "    if key == 32:  # SPACE\n",
    "        if success:\n",
    "            #Save full-frame overlay mask (green)\n",
    "            mask_full = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "            mask_full[y:y+h_box, x:x+w_box] = 255\n",
    "            rgb_mask_overlay = frame.copy()\n",
    "            rgb_mask_overlay[mask_full.astype(bool)] = rgb_mask_overlay[mask_full.astype(bool)] * 0.5 + np.array([0, 255, 0]) * 0.5\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"view_{capture_count:03d}_full.png\"), rgb_mask_overlay)\n",
    "\n",
    "            # SAM segmentation on full frame using bbox center\n",
    "            center_point_full = np.array([[x + w_box//2, y + h_box//2]])\n",
    "            predictor.set_image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            masks_full, _, _ = predictor.predict(\n",
    "                point_coords=center_point_full,\n",
    "                point_labels=np.array([1]),\n",
    "                multimask_output=True\n",
    "            )\n",
    "            mask_full_object = masks_full[0].astype(bool)\n",
    "\n",
    "            #RGB patch with background set to white\n",
    "            y_idx, x_idx = np.where(mask_full_object)\n",
    "            x0, x1 = x_idx.min(), x_idx.max()\n",
    "            y0, y1 = y_idx.min(), y_idx.max()\n",
    "            obj_crop = frame[y0:y1, x0:x1].copy()\n",
    "            mask_crop_bool = mask_full_object[y0:y1, x0:x1]\n",
    "\n",
    "            #Set background to white\n",
    "            obj_crop_white_bg = obj_crop.copy()\n",
    "            obj_crop_white_bg[~mask_crop_bool] = 255  # white background\n",
    "\n",
    "            #Save RGB patch and mask\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"view_{capture_count:03d}_crop.png\"), obj_crop_white_bg)\n",
    "\n",
    "            print(f\"Saved multi-view capture {capture_count:03d}\")\n",
    "            capture_count += 1\n",
    "\n",
    "    elif key == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e248e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annikaunmuessig/Developer/University/Group_Projects/Project_CV/.py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DINOv2 features for 3.0 potential perspectives...\n",
      "Library created with 3 feature vectors.\n",
      "\n",
      "--- Feature Preview (First 10 values per perspective) ---\n",
      "Perspective 0: [ 0.00556454  0.01795515 -0.0441304   0.0353582   0.04004984 -0.00626491\n",
      "  0.02437494 -0.03971501  0.00890234  0.03981451]\n",
      "Perspective 1: [ 0.06236326  0.0010755  -0.08177099  0.0025537  -0.04416817  0.02222248\n",
      " -0.00332737 -0.03287883 -0.08411137  0.08426923]\n",
      "Perspective 2: [ 0.00268321  0.0295081  -0.09203594 -0.05245358  0.05578673  0.04401093\n",
      " -0.07666223 -0.02480725 -0.08794063  0.08172967]\n"
     ]
    }
   ],
   "source": [
    "#PART 2: USING DINO TO EXTRACT FEATURES OF SCANNED OBJECT\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "#Setup DINOv2\n",
    "#'dinov2-small' is very fast; 'dinov2-base' or 'dinov2-giant' are more accurate\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small\")\n",
    "model = AutoModel.from_pretrained(\"facebook/dinov2-small\").to(device)\n",
    "\n",
    "def extract_features(folder_path): \n",
    "    instance_library = []\n",
    "    file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    print(f\"Extracting DINOv2 features for {len(file_list)/2} potential perspectives...\")\n",
    "    \n",
    "    for filename in file_list:\n",
    "        if \"full\" in filename:\n",
    "            continue\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # 4. Read and check if image exists\n",
    "        img_bgr = cv2.imread(full_path)\n",
    "        if img_bgr is None:\n",
    "            print(f\"Warning: Could not read {filename}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Process\n",
    "        crop_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(crop_rgb)\n",
    "        \n",
    "        inputs = processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)\n",
    "            instance_library.append(embeddings.cpu().numpy())\n",
    "            \n",
    "    return instance_library\n",
    "\n",
    "multi_view=\"multi_view\"\n",
    "instance_features = extract_features(multi_view)\n",
    "print(f\"Library created with {len(instance_features)} feature vectors.\")\n",
    "\n",
    "print(\"\\n--- Feature Preview (First 10 values per perspective) ---\")\n",
    "for i, feat in enumerate(instance_features):\n",
    "    # Flatten if necessary and take the first 10 elements\n",
    "    preview = feat.flatten()[:10]\n",
    "    print(f\"Perspective {i}: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a46018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 3 objects, 54.4ms\n",
      "Speed: 1.4ms preprocess, 54.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.090s\n",
      "FastSAM masks found: 3\n",
      "DINO embedding time: 0.028s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.029s\n",
      "TOTAL IDENTIFICATION TIME: 0.199s\n",
      "Target not found (best score=0.248)\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 18 objects, 71.7ms\n",
      "Speed: 3.3ms preprocess, 71.7ms inference, 26.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.104s\n",
      "FastSAM masks found: 18\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.029s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.023s\n",
      "TOTAL IDENTIFICATION TIME: 0.704s\n",
      "TARGET IDENTIFIED (score=0.716)\n",
      "Tracking lost.\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 7 objects, 47.4ms\n",
      "Speed: 1.0ms preprocess, 47.4ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.058s\n",
      "FastSAM masks found: 7\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.029s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "TOTAL IDENTIFICATION TIME: 0.292s\n",
      "Target not found (best score=0.216)\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 6 objects, 54.0ms\n",
      "Speed: 2.0ms preprocess, 54.0ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.064s\n",
      "FastSAM masks found: 6\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.028s\n",
      "DINO embedding time: 0.027s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "TOTAL IDENTIFICATION TIME: 0.270s\n",
      "Target not found (best score=0.159)\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 24 objects, 59.0ms\n",
      "Speed: 2.7ms preprocess, 59.0ms inference, 32.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.096s\n",
      "FastSAM masks found: 24\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.029s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "TOTAL IDENTIFICATION TIME: 0.892s\n",
      "Target not found (best score=0.367)\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 23 objects, 57.0ms\n",
      "Speed: 1.9ms preprocess, 57.0ms inference, 29.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.090s\n",
      "FastSAM masks found: 23\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.028s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.025s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "TOTAL IDENTIFICATION TIME: 0.855s\n",
      "Target not found (best score=0.498)\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 20 objects, 59.8ms\n",
      "Speed: 2.6ms preprocess, 59.8ms inference, 25.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.089s\n",
      "FastSAM masks found: 20\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.029s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.025s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.025s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "TOTAL IDENTIFICATION TIME: 0.757s\n",
      "TARGET IDENTIFIED (score=0.702)\n",
      "Tracking lost.\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 5 objects, 47.5ms\n",
      "Speed: 1.0ms preprocess, 47.5ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.056s\n",
      "FastSAM masks found: 5\n",
      "DINO embedding time: 0.029s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "TOTAL IDENTIFICATION TIME: 0.223s\n",
      "Target not found (best score=0.237)\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 2 objects, 70.6ms\n",
      "Speed: 3.0ms preprocess, 70.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.078s\n",
      "FastSAM masks found: 2\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.026s\n",
      "TOTAL IDENTIFICATION TIME: 0.146s\n",
      "Target not found (best score=0.236)\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 13 objects, 66.7ms\n",
      "Speed: 3.1ms preprocess, 66.7ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.090s\n",
      "FastSAM masks found: 13\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.031s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "TOTAL IDENTIFICATION TIME: 0.522s\n",
      "Target not found (best score=0.596)\n",
      "\n",
      "Running FastSAM + DINO identification\n",
      "\n",
      "0: 384x640 13 objects, 53.2ms\n",
      "Speed: 1.7ms preprocess, 53.2ms inference, 16.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FastSAM proposal time: 0.072s\n",
      "FastSAM masks found: 13\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.024s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.031s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.023s\n",
      "DINO embedding time: 0.022s\n",
      "DINO embedding time: 0.024s\n",
      "TOTAL IDENTIFICATION TIME: 0.504s\n",
      "TARGET IDENTIFIED (score=0.674)\n"
     ]
    }
   ],
   "source": [
    "#PART 3.1: IDENTIFYING OBJECT IN ROOM USING FASTSAM + DINO + TRACKING, MODIFIED VERSION\n",
    "#Modification: every 2 seconds the re-identification with FastSAM + DINO is performed again to correct potential tracking drifts\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "from ultralytics import FastSAM\n",
    "\n",
    "# ===============================\n",
    "# SETUP\n",
    "# ===============================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---- FastSAM ----\n",
    "fastsam = FastSAM(\"FastSAM-s.pt\")   # change path if needed\n",
    "\n",
    "# ---- DINOv2 ----\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small\")\n",
    "dino_model = AutoModel.from_pretrained(\"facebook/dinov2-small\").to(device)\n",
    "dino_model.eval()\n",
    "\n",
    "# ---- Webcam ----\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# ---- Tracking ----\n",
    "tracker = None\n",
    "tracking_active = False\n",
    "tracked_box = None\n",
    "\n",
    "SIM_THRESHOLD = 0.6\n",
    "REID_INTERVAL = 2.0   # seconds between re-ID attempts if tracking lost\n",
    "last_reid_time = 0\n",
    "\n",
    "# ===============================\n",
    "# MAIN LOOP\n",
    "# ===============================\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # =========================================================\n",
    "    # TRACKING ACTIVE → just track\n",
    "    # =========================================================\n",
    "    if tracking_active:\n",
    "        success, tracked_box = tracker.update(frame)\n",
    "\n",
    "        if success:\n",
    "            x, y, w, h = map(int, tracked_box)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "            cv2.putText(frame, \"TRACKING\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        else:\n",
    "            print(\"Tracking lost.\")\n",
    "            tracking_active = False  # object lost → allow re-ID\n",
    "\n",
    "    # =========================================================\n",
    "    # TRACKING LOST → re-run FastSAM + DINO every REID_INTERVAL\n",
    "    # =========================================================\n",
    "    if not tracking_active and (current_time - last_reid_time > REID_INTERVAL):\n",
    "        last_reid_time = current_time\n",
    "        print(\"\\nRunning FastSAM + DINO identification\")\n",
    "        total_start = time.time()\n",
    "\n",
    "        # ---- FastSAM ----\n",
    "        t_fastsam = time.time()\n",
    "        results = fastsam(\n",
    "            frame,\n",
    "            device=device,\n",
    "            imgsz=640,\n",
    "            conf=0.4,\n",
    "            iou=0.9,\n",
    "            retina_masks=True\n",
    "        )\n",
    "        print(f\"FastSAM proposal time: {time.time() - t_fastsam:.3f}s\")\n",
    "\n",
    "        masks_obj = results[0].masks\n",
    "        if masks_obj is None:\n",
    "            print(\"No masks found.\")\n",
    "            cv2.imshow(\"FastSAM + DINO → TRACKING\", frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == 27:  # ESC\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        masks_tensor = masks_obj.data\n",
    "        print(f\"FastSAM masks found: {masks_tensor.shape[0]}\")\n",
    "\n",
    "        best_match = {\"score\": 0, \"bbox\": None}\n",
    "\n",
    "        # ---- Loop over masks ----\n",
    "        for i in range(masks_tensor.shape[0]):\n",
    "            mask = masks_tensor[i].detach().cpu().numpy().astype(bool)\n",
    "            ys, xs = np.where(mask)\n",
    "            if len(xs) == 0 or len(ys) == 0:\n",
    "                continue\n",
    "\n",
    "            x1, x2 = xs.min(), xs.max()\n",
    "            y1, y2 = ys.min(), ys.max()\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            if w < 30 or h < 30:\n",
    "                continue\n",
    "\n",
    "            # isolate object\n",
    "            mask_3d = np.repeat(mask[:, :, None], 3, axis=2)\n",
    "            white_bg = np.ones_like(frame) * 255\n",
    "            isolated = np.where(mask_3d, frame, white_bg).astype(np.uint8)\n",
    "            crop = isolated[y1:y2, x1:x2]\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            # ---- DINO embedding ----\n",
    "            t_dino = time.time()\n",
    "            img_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "            inputs = processor(images=img_pil, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = dino_model(**inputs)\n",
    "\n",
    "            feat = outputs.last_hidden_state[:, 0, :]\n",
    "            feat = F.normalize(feat, dim=1)\n",
    "            candidate_emb = feat.cpu().numpy()\n",
    "            print(f\"DINO embedding time: {time.time() - t_dino:.3f}s\")\n",
    "\n",
    "            # ---- Similarity ----\n",
    "            sims = [\n",
    "                cosine_similarity(candidate_emb, ref.reshape(1, -1))[0][0]\n",
    "                for ref in instance_features\n",
    "            ]\n",
    "            max_sim = max(sims)\n",
    "\n",
    "            if max_sim > best_match[\"score\"]:\n",
    "                best_match = {\n",
    "                    \"score\": max_sim,\n",
    "                    \"bbox\": (x1, y1, w, h)\n",
    "                }\n",
    "\n",
    "        print(f\"TOTAL IDENTIFICATION TIME: {time.time() - total_start:.3f}s\")\n",
    "\n",
    "        # ---- Initialize tracker if confident ----\n",
    "        if best_match[\"score\"] > SIM_THRESHOLD:\n",
    "            print(f\"TARGET IDENTIFIED (score={best_match['score']:.3f})\")\n",
    "            tracker = cv2.legacy.TrackerCSRT_create()\n",
    "            tracker.init(frame, best_match[\"bbox\"])\n",
    "            tracked_box = best_match[\"bbox\"]\n",
    "            tracking_active = True\n",
    "        else:\n",
    "            print(f\"Target not found (best score={best_match['score']:.3f})\")\n",
    "            tracking_active = False\n",
    "\n",
    "    # =========================================================\n",
    "    # SHOW FRAME\n",
    "    # =========================================================\n",
    "    cv2.imshow(\"FastSAM + DINO → TRACKING\", frame)\n",
    "    key = cv2.waitKey(10) & 0xFF\n",
    "    if key == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
